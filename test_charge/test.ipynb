{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1befb0",
   "metadata": {},
   "source": [
    "# Tests de charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5cd51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "!pip install -q aiohttp\n",
    "!pip install -q nest_asyncio\n",
    "!pip install -q pyngrok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20d519",
   "metadata": {},
   "source": [
    "## Mesurer le temps de réponse moyen (latence) lorsque l'API reçoit un grand nombre de requêtes simultanées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f781d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Charge Basique ---\n",
      "5000 requêtes exécutées en 31.61s\n",
      "Débit (RPS) : 158.17\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# CORRECTION : Utiliser le point de terminaison de l'API, pas la page de documentation\n",
    "API_URL = \"http://34.38.214.124/analyze\" \n",
    "\n",
    "async def send_request(session, text ):\n",
    "    payload = {\"text\": text, \"model\": \"simple\"}\n",
    "    async with session.post(API_URL, json=payload) as r:\n",
    "        # Vérifier que la requête a réussi (Code 200)\n",
    "        if r.status != 200:\n",
    "            print(f\"Erreur HTTP: {r.status}\")\n",
    "        return await r.json()\n",
    "\n",
    "async def load_test(n_users=5000):\n",
    "    \"\"\"Simule 5000 utilisateurs envoyant une requête en même temps.\"\"\"\n",
    "    start = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, \"Hello world! This is a test comment.\") for _ in range(n_users)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    rps = n_users / duration\n",
    "    print(f\"\\n--- Test de Charge Basique ---\")\n",
    "    print(f\"{n_users} requêtes exécutées en {duration:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "\n",
    "await load_test()\n",
    "# asyncio.run(load_test())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e805191",
   "metadata": {},
   "source": [
    "## Déterminer la capacité maximale de l'API avant qu'elle ne commence à générer des erreurs (5xx) ou que la latence ne devienne inacceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0b286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Résistance (Stress Test) ---\n",
      "Charge: 100 requêtes | RPS: 91.72 | Erreurs: 0\n",
      "Charge: 200 requêtes | RPS: 155.25 | Erreurs: 0\n",
      "Charge: 300 requêtes | RPS: 164.49 | Erreurs: 0\n",
      "Charge: 400 requêtes | RPS: 166.39 | Erreurs: 0\n",
      "Charge: 500 requêtes | RPS: 128.73 | Erreurs: 0\n"
     ]
    }
   ],
   "source": [
    "async def stress_test(start_users=100, step=100, max_steps=5):\n",
    "    \"\"\"Augmente progressivement la charge pour trouver le point de rupture.\"\"\"\n",
    "    print(f\"\\n--- Test de Résistance (Stress Test) ---\")\n",
    "    for i in range(max_steps):\n",
    "        n_users = start_users + i * step\n",
    "        start = time.time()\n",
    "        \n",
    "        async with aiohttp.ClientSession( ) as session:\n",
    "            tasks = [send_request(session, \"Stress test payload.\") for _ in range(n_users)]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        rps = n_users / duration\n",
    "        \n",
    "        # Compter les erreurs (exceptions ou erreurs HTTP)\n",
    "        errors = sum(1 for r in results if isinstance(r, Exception) or (isinstance(r, dict) and 'error' in r))\n",
    "        \n",
    "        print(f\"Charge: {n_users} requêtes | RPS: {rps:.2f} | Erreurs: {errors}\")\n",
    "        \n",
    "        # Si le taux d'erreur dépasse 5%, on considère que le point de rupture est atteint\n",
    "        if errors / n_users > 0.05 and n_users > start_users:\n",
    "            print(f\"Point de rupture probable atteint à {n_users} requêtes.\")\n",
    "            break\n",
    "\n",
    "await stress_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922a1f0",
   "metadata": {},
   "source": [
    "## Détecter les problèmes de performance qui n'apparaissent qu'après une utilisation prolongée, comme les fuites de mémoire ou la dégradation des performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db563499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Durée (Soak Test) ---\n",
      "Temps écoulé: 0.1 min | Total requêtes: 50\n",
      "Temps écoulé: 0.2 min | Total requêtes: 100\n",
      "Temps écoulé: 0.3 min | Total requêtes: 150\n",
      "Temps écoulé: 0.3 min | Total requêtes: 200\n",
      "Temps écoulé: 0.4 min | Total requêtes: 250\n",
      "Temps écoulé: 0.5 min | Total requêtes: 300\n",
      "Temps écoulé: 0.6 min | Total requêtes: 350\n",
      "Temps écoulé: 0.7 min | Total requêtes: 400\n",
      "Temps écoulé: 0.8 min | Total requêtes: 450\n",
      "Temps écoulé: 0.8 min | Total requêtes: 500\n",
      "Temps écoulé: 0.9 min | Total requêtes: 550\n",
      "Temps écoulé: 1.0 min | Total requêtes: 600\n",
      "Temps écoulé: 1.1 min | Total requêtes: 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romarickaki/miniforge3/lib/python3.12/ast.py:278: RuntimeWarning: coroutine 'load_test' was never awaited\n",
      "  for item in field:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, interval_seconds - elapsed))\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTemps écoulé: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mduration_minutes\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m))\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min | Total requêtes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_requests\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m soak_test()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36msoak_test\u001b[39m\u001b[34m(duration_minutes, interval_seconds, users_per_interval)\u001b[39m\n\u001b[32m     14\u001b[39m elapsed = time.time() - start_interval\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Attendre pour maintenir l'intervalle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, interval_seconds - elapsed))\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTemps écoulé: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m(end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mduration_minutes\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m))\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min | Total requêtes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_requests\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/asyncio/tasks.py:665\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    661\u001b[39m h = loop.call_later(delay,\n\u001b[32m    662\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    663\u001b[39m                     future, result)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    667\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "async def soak_test(duration_minutes=10, interval_seconds=5, users_per_interval=50):\n",
    "    \"\"\"Maintient une charge constante pendant une longue période.\"\"\"\n",
    "    print(f\"\\n--- Test de Durée (Soak Test) ---\")\n",
    "    end_time = time.time() + duration_minutes * 60\n",
    "    total_requests = 0\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        start_interval = time.time()\n",
    "        async with aiohttp.ClientSession( ) as session:\n",
    "            tasks = [send_request(session, \"Soak test payload.\") for _ in range(users_per_interval)]\n",
    "            await asyncio.gather(*tasks)\n",
    "        \n",
    "        total_requests += users_per_interval\n",
    "        elapsed = time.time() - start_interval\n",
    "        \n",
    "        # Attendre pour maintenir l'intervalle\n",
    "        await asyncio.sleep(max(0, interval_seconds - elapsed))\n",
    "        \n",
    "        print(f\"Temps écoulé: {(time.time() - (end_time - duration_minutes * 60)) / 60:.1f} min | Total requêtes: {total_requests}\")\n",
    "\n",
    "await soak_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c20f4f",
   "metadata": {},
   "source": [
    "## Mesurer la différence de latence entre le traitement d'un texte court et simple et celui d'un texte long et complexe (avec beaucoup de ponctuation, d'entités nommées, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b76868",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scenario_test(n_requests=1000):\n",
    "    \"\"\"Compare la latence entre des requêtes simples et complexes.\"\"\"\n",
    "    \n",
    "    simple_text = \"Hello world.\"\n",
    "    complex_text = \"Dr. John Smith, a resident of 123 Main St, New York, NY, wrote a very long and complicated comment about the organization's policy on the 15th of March 2025.\"\n",
    "    \n",
    "    print(f\"\\n--- Test de Cas d'Usage (Scénario) ---\")\n",
    "    \n",
    "    # Scénario 1 : Texte Simple\n",
    "    start_simple = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, simple_text) for _ in range(n_requests)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_simple = time.time() - start_simple\n",
    "    print(f\"Simple ({n_requests} req): {duration_simple:.2f}s | Latence Moyenne: {duration_simple/n_requests*1000:.2f}ms\")\n",
    "\n",
    "    # Scénario 2 : Texte Complexe (met à l'épreuve l'anonymisation NLTK)\n",
    "    start_complex = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, complex_text) for _ in range(n_requests)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_complex = time.time() - start_complex\n",
    "    print(f\"Complexe ({n_requests} req): {duration_complex:.2f}s | Latence Moyenne: {duration_complex/n_requests*1000:.2f}ms\")\n",
    "\n",
    "await scenario_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea00c5",
   "metadata": {},
   "source": [
    "## Mesurer le temps de réponse de la toute première requête après une période d'inactivité (mise à l'échelle de 0 à 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cold_start_test():\n",
    "    \"\"\"Mesure le temps de réponse du premier appel (cold start) et des suivants.\"\"\"\n",
    "    print(f\"\\n--- Test de Montée en Charge à Froid (Cold Start) ---\")\n",
    "    \n",
    "    # Étape 1 : Le premier appel (Cold Start)\n",
    "    start_cold = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        await send_request(session, \"First call to wake up the service.\")\n",
    "    duration_cold = time.time() - start_cold\n",
    "    print(f\"Latence du Premier Appel (Cold Start) : {duration_cold:.2f}s\")\n",
    "\n",
    "    # Étape 2 : Les appels suivants (Warm Start)\n",
    "    n_warm = 100\n",
    "    start_warm = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, \"Warm call.\") for _ in range(n_warm)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_warm = time.time() - start_warm\n",
    "    avg_warm_latency = (duration_warm / n_warm) * 1000\n",
    "    print(f\"Latence Moyenne des {n_warm} Appels Suivants : {avg_warm_latency:.2f}ms\")\n",
    "    \n",
    "    # Évaluation : Le temps de Cold Start doit être significativement plus long que la latence moyenne.\n",
    "\n",
    "await cold_start_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00f750",
   "metadata": {},
   "source": [
    "## Vérifier que l'API ne plante pas (pas d'erreur 500) face à des textes très longs, des entrées vides, ou des entrées contenant uniquement des données personnelles (PII)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6373aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def edge_case_test():\n",
    "    \"\"\"Teste la résilience de l'API face à des entrées extrêmes.\"\"\"\n",
    "    print(f\"\\n--- Test de Cas Limites (Edge Case) ---\")\n",
    "    \n",
    "    # 1. Texte Extrêmement Long (simule un article entier)\n",
    "    long_text = \"A\" * 10000 + \" This is the end.\"\n",
    "    \n",
    "    # 2. Texte PII Pur (simule une tentative d'injection de données personnelles)\n",
    "    pii_text = \"My name is John Smith and my credit card is 1234567890123456. I live at 123 Main Street.\"\n",
    "    \n",
    "    # 3. Texte Vide\n",
    "    empty_text = \"\"\n",
    "    \n",
    "    test_cases = {\n",
    "        \"Long Text (10k chars)\": long_text,\n",
    "        \"PII Text (RGPD test)\": pii_text,\n",
    "        \"Empty Text\": empty_text\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        for name, text in test_cases.items():\n",
    "            start = time.time()\n",
    "            result = await send_request(session, text)\n",
    "            duration = time.time() - start\n",
    "            \n",
    "            status = \"SUCCÈS\" if result and 'social_score' in result else \"ÉCHEC (500)\"\n",
    "            \n",
    "            print(f\"Cas: {name} | Statut: {status} | Latence: {duration:.2f}s\")\n",
    "            \n",
    "            # Évaluation RGPD : Vérifier que le score est calculé et que le service n'a pas planté.\n",
    "            if name == \"PII Text (RGPD test)\" and status == \"SUCCÈS\":\n",
    "                print(f\"  -> Score obtenu: {result.get('social_score')}\")\n",
    "\n",
    "await edge_case_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49739e7e",
   "metadata": {},
   "source": [
    "## Simuler un pic de 1000 requêtes, mais en limitant la concurrence à 500 pour éviter de saturer le client de test et pour mesurer la performance sous une charge contrôlée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# HYPOTHÈSE : L'URL de l'API est http://34.38.214.124/analyze\n",
    "API_URL = \"http://34.38.214.124/analyze\" \n",
    "\n",
    "# --- Configuration du Test ---\n",
    "TOTAL_REQUESTS = 1000\n",
    "MAX_CONCURRENCY = 500\n",
    "TEST_TEXT = \"This is a test comment for the concurrent load test.\"\n",
    "\n",
    "# --- Fonctions de Test ---\n",
    "\n",
    "async def send_request_with_semaphore(session, text, semaphore ):\n",
    "    \"\"\"Envoie une requête, mais attend un jeton du sémaphore avant de commencer.\"\"\"\n",
    "    # Attendre qu'un des 500 slots de concurrence soit libre\n",
    "    async with semaphore:\n",
    "        start_time = time.time()\n",
    "        payload = {\"text\": text, \"model\": \"simple\"}\n",
    "        \n",
    "        async with session.post(API_URL, json=payload) as r:\n",
    "            # Lire la réponse pour s'assurer que le temps de réponse est complet\n",
    "            await r.read()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Retourner le temps de latence et le statut\n",
    "            return end_time - start_time, r.status\n",
    "\n",
    "async def concurrent_load_test():\n",
    "    print(f\"\\n--- Test de Charge (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    # Créer le sémaphore pour limiter la concurrence\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    \n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        # Créer toutes les tâches\n",
    "        tasks = [\n",
    "            send_request_with_semaphore(session, TEST_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        \n",
    "        # Exécuter les tâches, limitées par le sémaphore\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # --- Analyse des Résultats ---\n",
    "    latencies = [r[0] for r in results]\n",
    "    statuses = [r[1] for r in results]\n",
    "    \n",
    "    avg_latency = sum(latencies) / TOTAL_REQUESTS\n",
    "    rps = TOTAL_REQUESTS / duration_total\n",
    "    \n",
    "    errors = statuses.count(500) + statuses.count(400)\n",
    "    \n",
    "    print(f\"Total des requêtes exécutées : {TOTAL_REQUESTS}\")\n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "    print(f\"Latence Moyenne : {avg_latency * 1000:.2f}ms\")\n",
    "    print(f\"Nombre d'erreurs (4xx/5xx) : {errors}\")\n",
    "\n",
    "# Exécution du test\n",
    "await concurrent_load_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da0841",
   "metadata": {},
   "source": [
    "## Mesurer la latence et le débit lorsque les 500 workers simultanés traitent des données qui sollicitent fortement le CPU (anonymisation et tokenisation NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73491951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# HYPOTHÈSE : L'URL de l'API est http://34.38.214.124/analyze\n",
    "API_URL = \"http://34.38.214.124/analyze\" \n",
    "\n",
    "# --- Configuration du Test ---\n",
    "TOTAL_REQUESTS = 1000\n",
    "MAX_CONCURRENCY = 500\n",
    "# Texte complexe pour solliciter l'anonymisation (regex et NE chunking ) et la tokenisation\n",
    "COMPLEX_TEXT = \"Dr. John Smith, a resident of 123 Main St, New York, NY, wrote a very long and complicated comment about the organization's policy on the 15th of March 2025. This comment is highly toxic and offensive, and I hate it.\"\n",
    "\n",
    "# --- Fonctions de Test (Réutilisées) ---\n",
    "async def send_request_with_semaphore(session, text, semaphore):\n",
    "    # ... (même fonction que précédemment)\n",
    "    async with semaphore:\n",
    "        start_time = time.time()\n",
    "        payload = {\"text\": text, \"model\": \"simple\"}\n",
    "        \n",
    "        async with session.post(API_URL, json=payload) as r:\n",
    "            await r.read()\n",
    "            end_time = time.time()\n",
    "            return end_time - start_time, r.status\n",
    "\n",
    "async def complex_data_concurrency_test():\n",
    "    print(f\"\\n--- Test de Concurrence avec Données Complexes (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [\n",
    "            send_request_with_semaphore(session, COMPLEX_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # --- Analyse des Résultats ---\n",
    "    latencies = [r[0] for r in results]\n",
    "    avg_latency = sum(latencies) / TOTAL_REQUESTS\n",
    "    rps = TOTAL_REQUESTS / duration_total\n",
    "    \n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "    print(f\"Latence Moyenne : {avg_latency * 1000:.2f}ms\")\n",
    "\n",
    "await complex_data_concurrency_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47505572",
   "metadata": {},
   "source": [
    "## Mesurer l'impact de l'ajout d'un petit délai entre les requêtes sur la latence globale. Simule un client qui envoie des requêtes en rafale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_request_with_delay(session, text, semaphore):\n",
    "    # Ajout d'un micro-délai pour simuler un client qui n'est pas instantané\n",
    "    await asyncio.sleep(0.001) \n",
    "    return await send_request_with_semaphore(session, text, semaphore)\n",
    "\n",
    "async def throttled_concurrency_test():\n",
    "    print(f\"\\n--- Test de Concurrence avec Délai (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [\n",
    "            send_request_with_delay(session, TEST_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # ... (Analyse des Résultats identique à la Variation 1)\n",
    "    latencies = [r[0] for r in results]\n",
    "    avg_latency = sum(latencies) / TOTAL_REQUESTS\n",
    "    rps = TOTAL_REQUESTS / duration_total\n",
    "    \n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "    print(f\"Latence Moyenne : {avg_latency * 1000:.2f}ms\")\n",
    "\n",
    "await throttled_concurrency_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fd156",
   "metadata": {},
   "source": [
    "## Mesurer le taux d'erreur (codes 5xx) et la capacité du système à se stabiliser après un pic de charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bedad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def resilience_concurrency_test():\n",
    "    print(f\"\\n--- Test de Concurrence avec Évaluation des Erreurs (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [\n",
    "            send_request_with_semaphore(session, TEST_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # --- Analyse des Résultats ---\n",
    "    statuses = [r[1] for r in results]\n",
    "    \n",
    "    # Compter les erreurs\n",
    "    errors_5xx = statuses.count(500) + statuses.count(502) + statuses.count(503)\n",
    "    errors_4xx = statuses.count(400) + statuses.count(404)\n",
    "    \n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Taux de Succès : {100 - (errors_5xx + errors_4xx) / TOTAL_REQUESTS * 100:.2f}%\")\n",
    "    print(f\"Erreurs Serveur (5xx) : {errors_5xx}\")\n",
    "    print(f\"Erreurs Client (4xx) : {errors_4xx}\")\n",
    "\n",
    "await resilience_concurrency_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a264b17",
   "metadata": {},
   "source": [
    "## 1. Montée en Charge Progressive (Ramp-up Test)\n",
    "Objectif\n",
    "Évaluation\n",
    "Mesurer comment le système réagit à une augmentation constante et progressive de la charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7880f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def progressive_ramp_up_test(max_concurrency=500, step=50, step_duration=10):\n",
    "    \"\"\"Augmente la charge de 50 utilisateurs toutes les 10 secondes jusqu'à 500.\"\"\"\n",
    "    print(\"\\n--- 1. Montée en Charge Progressive ---\")\n",
    "    \n",
    "    current_concurrency = 0\n",
    "    while current_concurrency <= max_concurrency:\n",
    "        current_concurrency += step\n",
    "        if current_concurrency > max_concurrency:\n",
    "            current_concurrency = max_concurrency\n",
    "            \n",
    "        print(f\"Charge actuelle: {current_concurrency} utilisateurs pendant {step_duration}s\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        semaphore = asyncio.Semaphore(current_concurrency)\n",
    "        \n",
    "        async with aiohttp.ClientSession( ) as session:\n",
    "            # Créer un grand nombre de tâches pour s'assurer que le sémaphore est toujours plein\n",
    "            tasks = [send_request_with_semaphore(session, \"Ramp-up test.\") for _ in range(current_concurrency * 5)]\n",
    "            \n",
    "            # Exécuter pendant la durée de l'étape\n",
    "            await asyncio.wait(tasks, timeout=step_duration)\n",
    "            \n",
    "        # Analyser les métriques ici (RPS, Latence)\n",
    "        # ...\n",
    "        \n",
    "        if current_concurrency == max_concurrency:\n",
    "            break\n",
    "\n",
    "await progressive_ramp_up_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3e9fd",
   "metadata": {},
   "source": [
    "## Détecter les fuites de mémoire, la dégradation des performances ou les problèmes de garbage collection sur une longue période."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Utilisez le code soak_test de la réponse précédente en fixant duration_minutes=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005aaf3c",
   "metadata": {},
   "source": [
    "## Vérifier la capacité du système à absorber un pic de charge soudain et à se rétablir rapidement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e118c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def spike_test(spike_concurrency=1000, normal_concurrency=50):\n",
    "    \"\"\"Simule un pic soudain de 1000 requêtes.\"\"\"\n",
    "    print(\"\\n--- 3. Burst Trafic Soudain (Spike Test) ---\")\n",
    "    \n",
    "    # Phase 1 : Le Spike (1000 requêtes simultanées)\n",
    "    print(f\"Phase 1: Spike de {spike_concurrency} requêtes...\")\n",
    "    start_spike = time.time()\n",
    "    semaphore = asyncio.Semaphore(spike_concurrency)\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request_with_semaphore(session, \"Spike payload.\", semaphore) for _ in range(spike_concurrency)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    print(f\"Spike terminé en {time.time() - start_spike:.2f}s\")\n",
    "    \n",
    "    # Phase 2 : Récupération (Charge nominale)\n",
    "    print(f\"Phase 2: Vérification de la récupération ({normal_concurrency} requêtes)...\")\n",
    "    start_recovery = time.time()\n",
    "    semaphore = asyncio.Semaphore(normal_concurrency)\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request_with_semaphore(session, \"Recovery payload.\", semaphore) for _ in range(normal_concurrency)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    print(f\"Récupération terminée en {time.time() - start_recovery:.2f}s\")\n",
    "    \n",
    "    # Évaluation : La latence de la Phase 2 doit être proche de la latence normale.\n",
    "\n",
    "await spike_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5289b2",
   "metadata": {},
   "source": [
    "## 8. Traitement Payloads Lourds (Large JSON)\n",
    "Objectif\n",
    "\n",
    "Code (Logique)\n",
    "Mesurer l'impact du transfert de données volumineuses (bien que le texte soit généralement petit, nous simulons un grand corps de requête)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def large_payload_test(n_requests=100):\n",
    "    \"\"\"Teste l'impact d'un corps de requête volumineux.\"\"\"\n",
    "    print(\"\\n--- 8. Traitement Payloads Lourds ---\")\n",
    "    \n",
    "    # Créer un texte de 1 Mo (environ 1 million de caractères)\n",
    "    large_text = \"A\" * 1000000 \n",
    "    \n",
    "    start_time = time.time()\n",
    "    semaphore = asyncio.Semaphore(10) # Limiter la concurrence pour ne pas saturer\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request_with_semaphore(session, large_text, semaphore) for _ in range(n_requests)]\n",
    "        await asyncio.gather(*tasks)\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{n_requests} requêtes de 1 Mo exécutées en {duration:.2f}s\")\n",
    "    \n",
    "    # Évaluation : La latence doit rester acceptable. Surveiller l'utilisation de la mémoire dans Cloud Monitoring.\n",
    "\n",
    "await large_payload_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
