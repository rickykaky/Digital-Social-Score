{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1befb0",
   "metadata": {},
   "source": [
    "# Tests de charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5cd51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "!pip install -q aiohttp\n",
    "!pip install -q nest_asyncio\n",
    "!pip install -q pyngrok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20d519",
   "metadata": {},
   "source": [
    "## Mesurer le temps de réponse moyen (latence) lorsque l'API reçoit un grand nombre de requêtes simultanées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f781d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romarickaki/miniforge3/lib/python3.12/site-packages/aiohttp/client.py:1509: RuntimeWarning: coroutine 'load_test' was never awaited\n",
      "  async def __aenter__(self) -> _RetType:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Charge Basique ---\n",
      "5000 requêtes exécutées en 25.97s\n",
      "Débit (RPS) : 192.52\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# CORRECTION : Utiliser le point de terminaison de l'API, pas la page de documentation\n",
    "API_URL = \"http://34.38.214.124/analyze\" \n",
    "\n",
    "async def send_request(session, text ):\n",
    "    payload = {\"text\": text, \"model\": \"simple\"}\n",
    "    async with session.post(API_URL, json=payload) as r:\n",
    "        # Vérifier que la requête a réussi (Code 200)\n",
    "        if r.status != 200:\n",
    "            print(f\"Erreur HTTP: {r.status}\")\n",
    "        return await r.json()\n",
    "\n",
    "async def load_test(n_users=5000):\n",
    "    \"\"\"Simule 5000 utilisateurs envoyant une requête en même temps.\"\"\"\n",
    "    start = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, \"Hello world! This is a test comment.\") for _ in range(n_users)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    rps = n_users / duration\n",
    "    print(f\"\\n--- Test de Charge Basique ---\")\n",
    "    print(f\"{n_users} requêtes exécutées en {duration:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "\n",
    "await load_test()\n",
    "# asyncio.run(load_test())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e805191",
   "metadata": {},
   "source": [
    "## Déterminer la capacité maximale de l'API avant qu'elle ne commence à générer des erreurs (5xx) ou que la latence ne devienne inacceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0b286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Résistance (Stress Test) ---\n",
      "Charge: 100 requêtes | RPS: 91.72 | Erreurs: 0\n",
      "Charge: 200 requêtes | RPS: 155.25 | Erreurs: 0\n",
      "Charge: 300 requêtes | RPS: 164.49 | Erreurs: 0\n",
      "Charge: 400 requêtes | RPS: 166.39 | Erreurs: 0\n",
      "Charge: 500 requêtes | RPS: 128.73 | Erreurs: 0\n"
     ]
    }
   ],
   "source": [
    "async def stress_test(start_users=100, step=100, max_steps=5):\n",
    "    \"\"\"Augmente progressivement la charge pour trouver le point de rupture.\"\"\"\n",
    "    print(f\"\\n--- Test de Résistance (Stress Test) ---\")\n",
    "    for i in range(max_steps):\n",
    "        n_users = start_users + i * step\n",
    "        start = time.time()\n",
    "        \n",
    "        async with aiohttp.ClientSession( ) as session:\n",
    "            tasks = [send_request(session, \"Stress test payload.\") for _ in range(n_users)]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        rps = n_users / duration\n",
    "        \n",
    "        # Compter les erreurs (exceptions ou erreurs HTTP)\n",
    "        errors = sum(1 for r in results if isinstance(r, Exception) or (isinstance(r, dict) and 'error' in r))\n",
    "        \n",
    "        print(f\"Charge: {n_users} requêtes | RPS: {rps:.2f} | Erreurs: {errors}\")\n",
    "        \n",
    "        # Si le taux d'erreur dépasse 5%, on considère que le point de rupture est atteint\n",
    "        if errors / n_users > 0.05 and n_users > start_users:\n",
    "            print(f\"Point de rupture probable atteint à {n_users} requêtes.\")\n",
    "            break\n",
    "\n",
    "await stress_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922a1f0",
   "metadata": {},
   "source": [
    "## Détecter les problèmes de performance qui n'apparaissent qu'après une utilisation prolongée, comme les fuites de mémoire ou la dégradation des performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db563499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Durée (Soak Test) ---\n",
      "Temps écoulé: 0.1 min | Total requêtes: 50\n",
      "Temps écoulé: 0.2 min | Total requêtes: 100\n",
      "Temps écoulé: 0.3 min | Total requêtes: 150\n",
      "Temps écoulé: 0.4 min | Total requêtes: 200\n",
      "Temps écoulé: 0.5 min | Total requêtes: 250\n",
      "Temps écoulé: 0.5 min | Total requêtes: 300\n",
      "Temps écoulé: 0.6 min | Total requêtes: 350\n",
      "Temps écoulé: 0.7 min | Total requêtes: 400\n",
      "Temps écoulé: 0.8 min | Total requêtes: 450\n",
      "Temps écoulé: 0.9 min | Total requêtes: 500\n",
      "Temps écoulé: 1.0 min | Total requêtes: 550\n",
      "Temps écoulé: 1.0 min | Total requêtes: 600\n",
      "Temps écoulé: 1.1 min | Total requêtes: 650\n",
      "Temps écoulé: 1.2 min | Total requêtes: 700\n",
      "Temps écoulé: 1.3 min | Total requêtes: 750\n",
      "Temps écoulé: 1.4 min | Total requêtes: 800\n",
      "Temps écoulé: 1.5 min | Total requêtes: 850\n",
      "Temps écoulé: 1.5 min | Total requêtes: 900\n",
      "Temps écoulé: 1.6 min | Total requêtes: 950\n",
      "Temps écoulé: 1.7 min | Total requêtes: 1000\n",
      "Temps écoulé: 1.8 min | Total requêtes: 1050\n",
      "Temps écoulé: 1.9 min | Total requêtes: 1100\n",
      "Temps écoulé: 2.0 min | Total requêtes: 1150\n",
      "Temps écoulé: 2.0 min | Total requêtes: 1200\n",
      "Temps écoulé: 2.1 min | Total requêtes: 1250\n",
      "Temps écoulé: 2.2 min | Total requêtes: 1300\n",
      "Temps écoulé: 2.3 min | Total requêtes: 1350\n",
      "Temps écoulé: 2.4 min | Total requêtes: 1400\n",
      "Temps écoulé: 2.5 min | Total requêtes: 1450\n",
      "Temps écoulé: 2.5 min | Total requêtes: 1500\n",
      "Temps écoulé: 2.6 min | Total requêtes: 1550\n",
      "Temps écoulé: 2.7 min | Total requêtes: 1600\n",
      "Temps écoulé: 2.8 min | Total requêtes: 1650\n",
      "Temps écoulé: 2.9 min | Total requêtes: 1700\n",
      "Temps écoulé: 3.0 min | Total requêtes: 1750\n",
      "Temps écoulé: 3.0 min | Total requêtes: 1800\n",
      "Temps écoulé: 3.1 min | Total requêtes: 1850\n",
      "Temps écoulé: 3.2 min | Total requêtes: 1900\n",
      "Temps écoulé: 3.3 min | Total requêtes: 1950\n",
      "Temps écoulé: 3.4 min | Total requêtes: 2000\n",
      "Temps écoulé: 3.5 min | Total requêtes: 2050\n",
      "Temps écoulé: 3.5 min | Total requêtes: 2100\n",
      "Temps écoulé: 3.6 min | Total requêtes: 2150\n",
      "Temps écoulé: 3.7 min | Total requêtes: 2200\n",
      "Temps écoulé: 3.8 min | Total requêtes: 2250\n",
      "Temps écoulé: 3.9 min | Total requêtes: 2300\n",
      "Temps écoulé: 4.0 min | Total requêtes: 2350\n",
      "Temps écoulé: 4.0 min | Total requêtes: 2400\n",
      "Temps écoulé: 4.1 min | Total requêtes: 2450\n",
      "Temps écoulé: 4.2 min | Total requêtes: 2500\n",
      "Temps écoulé: 4.3 min | Total requêtes: 2550\n",
      "Temps écoulé: 4.4 min | Total requêtes: 2600\n",
      "Temps écoulé: 4.5 min | Total requêtes: 2650\n",
      "Temps écoulé: 4.5 min | Total requêtes: 2700\n",
      "Temps écoulé: 4.6 min | Total requêtes: 2750\n",
      "Temps écoulé: 4.7 min | Total requêtes: 2800\n",
      "Temps écoulé: 4.8 min | Total requêtes: 2850\n",
      "Temps écoulé: 4.9 min | Total requêtes: 2900\n",
      "Temps écoulé: 5.0 min | Total requêtes: 2950\n",
      "Temps écoulé: 5.0 min | Total requêtes: 3000\n",
      "Temps écoulé: 5.1 min | Total requêtes: 3050\n",
      "Temps écoulé: 5.2 min | Total requêtes: 3100\n",
      "Temps écoulé: 5.3 min | Total requêtes: 3150\n",
      "Temps écoulé: 5.4 min | Total requêtes: 3200\n",
      "Temps écoulé: 5.5 min | Total requêtes: 3250\n",
      "Temps écoulé: 5.5 min | Total requêtes: 3300\n",
      "Temps écoulé: 5.6 min | Total requêtes: 3350\n",
      "Temps écoulé: 5.7 min | Total requêtes: 3400\n",
      "Temps écoulé: 5.8 min | Total requêtes: 3450\n",
      "Temps écoulé: 5.9 min | Total requêtes: 3500\n",
      "Temps écoulé: 6.0 min | Total requêtes: 3550\n",
      "Temps écoulé: 6.0 min | Total requêtes: 3600\n",
      "Temps écoulé: 6.1 min | Total requêtes: 3650\n",
      "Temps écoulé: 6.2 min | Total requêtes: 3700\n",
      "Temps écoulé: 6.3 min | Total requêtes: 3750\n",
      "Temps écoulé: 6.4 min | Total requêtes: 3800\n",
      "Temps écoulé: 6.5 min | Total requêtes: 3850\n",
      "Temps écoulé: 6.5 min | Total requêtes: 3900\n",
      "Temps écoulé: 6.6 min | Total requêtes: 3950\n",
      "Temps écoulé: 6.7 min | Total requêtes: 4000\n",
      "Temps écoulé: 6.8 min | Total requêtes: 4050\n",
      "Temps écoulé: 6.9 min | Total requêtes: 4100\n",
      "Temps écoulé: 7.0 min | Total requêtes: 4150\n",
      "Temps écoulé: 7.0 min | Total requêtes: 4200\n",
      "Temps écoulé: 7.1 min | Total requêtes: 4250\n",
      "Temps écoulé: 7.2 min | Total requêtes: 4300\n",
      "Temps écoulé: 7.3 min | Total requêtes: 4350\n",
      "Temps écoulé: 7.4 min | Total requêtes: 4400\n",
      "Temps écoulé: 7.5 min | Total requêtes: 4450\n",
      "Temps écoulé: 7.5 min | Total requêtes: 4500\n",
      "Temps écoulé: 7.6 min | Total requêtes: 4550\n",
      "Temps écoulé: 7.7 min | Total requêtes: 4600\n",
      "Temps écoulé: 7.8 min | Total requêtes: 4650\n",
      "Temps écoulé: 7.9 min | Total requêtes: 4700\n",
      "Temps écoulé: 8.0 min | Total requêtes: 4750\n",
      "Temps écoulé: 8.0 min | Total requêtes: 4800\n",
      "Temps écoulé: 8.1 min | Total requêtes: 4850\n",
      "Temps écoulé: 8.2 min | Total requêtes: 4900\n",
      "Temps écoulé: 8.3 min | Total requêtes: 4950\n",
      "Temps écoulé: 8.4 min | Total requêtes: 5000\n",
      "Temps écoulé: 8.5 min | Total requêtes: 5050\n",
      "Temps écoulé: 8.5 min | Total requêtes: 5100\n",
      "Temps écoulé: 8.6 min | Total requêtes: 5150\n",
      "Temps écoulé: 8.7 min | Total requêtes: 5200\n",
      "Temps écoulé: 8.8 min | Total requêtes: 5250\n",
      "Temps écoulé: 8.9 min | Total requêtes: 5300\n",
      "Temps écoulé: 9.0 min | Total requêtes: 5350\n",
      "Temps écoulé: 9.0 min | Total requêtes: 5400\n",
      "Temps écoulé: 9.1 min | Total requêtes: 5450\n",
      "Temps écoulé: 9.2 min | Total requêtes: 5500\n",
      "Temps écoulé: 9.3 min | Total requêtes: 5550\n",
      "Temps écoulé: 9.4 min | Total requêtes: 5600\n",
      "Temps écoulé: 9.5 min | Total requêtes: 5650\n",
      "Temps écoulé: 9.5 min | Total requêtes: 5700\n",
      "Temps écoulé: 9.6 min | Total requêtes: 5750\n",
      "Temps écoulé: 9.7 min | Total requêtes: 5800\n",
      "Temps écoulé: 9.8 min | Total requêtes: 5850\n",
      "Temps écoulé: 9.9 min | Total requêtes: 5900\n",
      "Temps écoulé: 10.0 min | Total requêtes: 5950\n",
      "Temps écoulé: 10.0 min | Total requêtes: 6000\n"
     ]
    }
   ],
   "source": [
    "async def soak_test(duration_minutes=10, interval_seconds=5, users_per_interval=50):\n",
    "    \"\"\"Maintient une charge constante pendant une longue période.\"\"\"\n",
    "    print(f\"\\n--- Test de Durée (Soak Test) ---\")\n",
    "    end_time = time.time() + duration_minutes * 60\n",
    "    total_requests = 0\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        start_interval = time.time()\n",
    "        async with aiohttp.ClientSession( ) as session:\n",
    "            tasks = [send_request(session, \"Soak test payload.\") for _ in range(users_per_interval)]\n",
    "            await asyncio.gather(*tasks)\n",
    "        \n",
    "        total_requests += users_per_interval\n",
    "        elapsed = time.time() - start_interval\n",
    "        \n",
    "        # Attendre pour maintenir l'intervalle\n",
    "        await asyncio.sleep(max(0, interval_seconds - elapsed))\n",
    "        \n",
    "        print(f\"Temps écoulé: {(time.time() - (end_time - duration_minutes * 60)) / 60:.1f} min | Total requêtes: {total_requests}\")\n",
    "\n",
    "await soak_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c20f4f",
   "metadata": {},
   "source": [
    "## Mesurer la différence de latence entre le traitement d'un texte court et simple et celui d'un texte long et complexe (avec beaucoup de ponctuation, d'entités nommées, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b76868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Cas d'Usage (Scénario) ---\n",
      "Simple (1000 req): 6.85s | Latence Moyenne: 6.85ms\n",
      "Complexe (1000 req): 8.07s | Latence Moyenne: 8.07ms\n"
     ]
    }
   ],
   "source": [
    "async def scenario_test(n_requests=1000):\n",
    "    \"\"\"Compare la latence entre des requêtes simples et complexes.\"\"\"\n",
    "    \n",
    "    simple_text = \"Derrick Feliho, a very bad student at the University of Paris\"\n",
    "    complex_text = \"Dr. John Smith, a resident of 123 Main St, New York, NY, wrote a very long and complicated fake comment about the organization's policy on the 15th of March 2025 and he insult all the students in this school by motherfucker.\"\n",
    "\n",
    "    print(f\"\\n--- Test de Cas d'Usage (Scénario) ---\")\n",
    "    \n",
    "    # Scénario 1 : Texte Simple\n",
    "    start_simple = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, simple_text) for _ in range(n_requests)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_simple = time.time() - start_simple\n",
    "    print(f\"Simple ({n_requests} req): {duration_simple:.2f}s | Latence Moyenne: {duration_simple/n_requests*1000:.2f}ms\")\n",
    "\n",
    "    # Scénario 2 : Texte Complexe (met à l'épreuve l'anonymisation NLTK)\n",
    "    start_complex = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, complex_text) for _ in range(n_requests)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_complex = time.time() - start_complex\n",
    "    print(f\"Complexe ({n_requests} req): {duration_complex:.2f}s | Latence Moyenne: {duration_complex/n_requests*1000:.2f}ms\")\n",
    "\n",
    "await scenario_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea00c5",
   "metadata": {},
   "source": [
    "## Mesurer le temps de réponse de la toute première requête après une période d'inactivité (mise à l'échelle de 0 à 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f66e0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Montée en Charge à Froid (Cold Start) ---\n",
      "Latence du Premier Appel (Cold Start) : 0.07s\n",
      "Latence Moyenne des 100 Appels Suivants : 17.13ms\n"
     ]
    }
   ],
   "source": [
    "async def cold_start_test():\n",
    "    \"\"\"Mesure le temps de réponse du premier appel (cold start) et des suivants.\"\"\"\n",
    "    print(f\"\\n--- Test de Montée en Charge à Froid (Cold Start) ---\")\n",
    "    \n",
    "    # Étape 1 : Le premier appel (Cold Start)\n",
    "    start_cold = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        await send_request(session, \"First call to wake up the service.\")\n",
    "    duration_cold = time.time() - start_cold\n",
    "    print(f\"Latence du Premier Appel (Cold Start) : {duration_cold:.2f}s\")\n",
    "\n",
    "    # Étape 2 : Les appels suivants (Warm Start)\n",
    "    n_warm = 100\n",
    "    start_warm = time.time()\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request(session, \"Warm call.\") for _ in range(n_warm)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_warm = time.time() - start_warm\n",
    "    avg_warm_latency = (duration_warm / n_warm) * 1000\n",
    "    print(f\"Latence Moyenne des {n_warm} Appels Suivants : {avg_warm_latency:.2f}ms\")\n",
    "    \n",
    "    # Évaluation : Le temps de Cold Start doit être significativement plus long que la latence moyenne.\n",
    "\n",
    "await cold_start_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00f750",
   "metadata": {},
   "source": [
    "## Vérifier que l'API ne plante pas (pas d'erreur 500) face à des textes très longs, des entrées vides, ou des entrées contenant uniquement des données personnelles (PII)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6373aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Cas Limites (Edge Case) ---\n",
      "Erreur HTTP: 422\n",
      "Cas: Long Text (10k chars) | Statut: ÉCHEC (500) | Latence: 0.09s\n",
      "Cas: PII Text (RGPD test) | Statut: ÉCHEC (500) | Latence: 0.10s\n",
      "Erreur HTTP: 422\n",
      "Cas: Empty Text | Statut: ÉCHEC (500) | Latence: 0.10s\n"
     ]
    }
   ],
   "source": [
    "async def edge_case_test():\n",
    "    \"\"\"Teste la résilience de l'API face à des entrées extrêmes.\"\"\"\n",
    "    print(f\"\\n--- Test de Cas Limites (Edge Case) ---\")\n",
    "    \n",
    "    # 1. Texte Extrêmement Long (simule un article entier)\n",
    "    long_text = \"A\" * 10000 + \" This is the end.\"\n",
    "    \n",
    "    # 2. Texte PII Pur (simule une tentative d'injection de données personnelles)\n",
    "    pii_text = \"My name is John Smith and my credit card is 1234567890123456. I live at 123 Main Street.\"\n",
    "    \n",
    "    # 3. Texte Vide\n",
    "    empty_text = \"\"\n",
    "    \n",
    "    test_cases = {\n",
    "        \"Long Text (10k chars)\": long_text,\n",
    "        \"PII Text (RGPD test)\": pii_text,\n",
    "        \"Empty Text\": empty_text\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        for name, text in test_cases.items():\n",
    "            start = time.time()\n",
    "            result = await send_request(session, text)\n",
    "            duration = time.time() - start\n",
    "            \n",
    "            status = \"SUCCÈS\" if result and 'social_score' in result else \"ÉCHEC (500)\"\n",
    "            \n",
    "            print(f\"Cas: {name} | Statut: {status} | Latence: {duration:.2f}s\")\n",
    "            \n",
    "            # Évaluation RGPD : Vérifier que le score est calculé et que le service n'a pas planté.\n",
    "            if name == \"PII Text (RGPD test)\" and status == \"SUCCÈS\":\n",
    "                print(f\"  -> Score obtenu: {result.get('social_score')}\")\n",
    "\n",
    "await edge_case_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49739e7e",
   "metadata": {},
   "source": [
    "## Simuler un pic de 1000 requêtes, mais en limitant la concurrence à 500 pour éviter de saturer le client de test et pour mesurer la performance sous une charge contrôlée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "742f110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Charge (Total: 1000 | Simultanéité: 500) ---\n",
      "Total des requêtes exécutées : 1000\n",
      "Durée totale : 6.81s\n",
      "Débit (RPS) : 146.76\n",
      "Latence Moyenne : 2586.52ms\n",
      "Nombre d'erreurs (4xx/5xx) : 0\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# HYPOTHÈSE : L'URL de l'API est http://34.38.214.124/analyze\n",
    "API_URL = \"http://34.38.214.124/analyze\" \n",
    "\n",
    "# --- Configuration du Test ---\n",
    "TOTAL_REQUESTS = 1000\n",
    "MAX_CONCURRENCY = 500\n",
    "TEST_TEXT = \"This is a test comment for the concurrent load test.\"\n",
    "\n",
    "# --- Fonctions de Test ---\n",
    "\n",
    "async def send_request_with_semaphore(session, text, semaphore ):\n",
    "    \"\"\"Envoie une requête, mais attend un jeton du sémaphore avant de commencer.\"\"\"\n",
    "    # Attendre qu'un des 500 slots de concurrence soit libre\n",
    "    async with semaphore:\n",
    "        start_time = time.time()\n",
    "        payload = {\"text\": text, \"model\": \"simple\"}\n",
    "        \n",
    "        async with session.post(API_URL, json=payload) as r:\n",
    "            # Lire la réponse pour s'assurer que le temps de réponse est complet\n",
    "            await r.read()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Retourner le temps de latence et le statut\n",
    "            return end_time - start_time, r.status\n",
    "\n",
    "async def concurrent_load_test():\n",
    "    print(f\"\\n--- Test de Charge (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    # Créer le sémaphore pour limiter la concurrence\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    \n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        # Créer toutes les tâches\n",
    "        tasks = [\n",
    "            send_request_with_semaphore(session, TEST_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        \n",
    "        # Exécuter les tâches, limitées par le sémaphore\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # --- Analyse des Résultats ---\n",
    "    latencies = [r[0] for r in results]\n",
    "    statuses = [r[1] for r in results]\n",
    "    \n",
    "    avg_latency = sum(latencies) / TOTAL_REQUESTS\n",
    "    rps = TOTAL_REQUESTS / duration_total\n",
    "    \n",
    "    errors = statuses.count(500) + statuses.count(400)\n",
    "    \n",
    "    print(f\"Total des requêtes exécutées : {TOTAL_REQUESTS}\")\n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "    print(f\"Latence Moyenne : {avg_latency * 1000:.2f}ms\")\n",
    "    print(f\"Nombre d'erreurs (4xx/5xx) : {errors}\")\n",
    "\n",
    "# Exécution du test\n",
    "await concurrent_load_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da0841",
   "metadata": {},
   "source": [
    "## Mesurer la latence et le débit lorsque les 500 workers simultanés traitent des données qui sollicitent fortement le CPU (anonymisation et tokenisation NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73491951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Concurrence avec Données Complexes (Total: 1000 | Simultanéité: 500) ---\n",
      "Durée totale : 7.84s\n",
      "Débit (RPS) : 127.56\n",
      "Latence Moyenne : 3254.56ms\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# HYPOTHÈSE : L'URL de l'API est http://34.38.214.124/analyze\n",
    "API_URL = \"http://34.38.214.124/analyze\" \n",
    "\n",
    "# --- Configuration du Test ---\n",
    "TOTAL_REQUESTS = 1000\n",
    "MAX_CONCURRENCY = 500\n",
    "# Texte complexe pour solliciter l'anonymisation (regex et NE chunking ) et la tokenisation\n",
    "COMPLEX_TEXT = \"Dr. John Smith, a resident of 123 Main St, New York, NY, wrote a very long and complicated comment about the organization's policy on the 15th of March 2025. This comment is highly toxic and offensive, and I hate it.\"\n",
    "\n",
    "# --- Fonctions de Test (Réutilisées) ---\n",
    "async def send_request_with_semaphore(session, text, semaphore):\n",
    "    # ... (même fonction que précédemment)\n",
    "    async with semaphore:\n",
    "        start_time = time.time()\n",
    "        payload = {\"text\": text, \"model\": \"simple\"}\n",
    "        \n",
    "        async with session.post(API_URL, json=payload) as r:\n",
    "            await r.read()\n",
    "            end_time = time.time()\n",
    "            return end_time - start_time, r.status\n",
    "\n",
    "async def complex_data_concurrency_test():\n",
    "    print(f\"\\n--- Test de Concurrence avec Données Complexes (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [\n",
    "            send_request_with_semaphore(session, COMPLEX_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # --- Analyse des Résultats ---\n",
    "    latencies = [r[0] for r in results]\n",
    "    avg_latency = sum(latencies) / TOTAL_REQUESTS\n",
    "    rps = TOTAL_REQUESTS / duration_total\n",
    "    \n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "    print(f\"Latence Moyenne : {avg_latency * 1000:.2f}ms\")\n",
    "\n",
    "await complex_data_concurrency_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47505572",
   "metadata": {},
   "source": [
    "## Mesurer l'impact de l'ajout d'un petit délai entre les requêtes sur la latence globale. Simule un client qui envoie des requêtes en rafale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8abcb7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Concurrence avec Délai (Total: 1000 | Simultanéité: 500) ---\n",
      "Durée totale : 6.61s\n",
      "Débit (RPS) : 151.32\n",
      "Latence Moyenne : 2638.96ms\n"
     ]
    }
   ],
   "source": [
    "async def send_request_with_delay(session, text, semaphore):\n",
    "    # Ajout d'un micro-délai pour simuler un client qui n'est pas instantané\n",
    "    await asyncio.sleep(0.001) \n",
    "    return await send_request_with_semaphore(session, text, semaphore)\n",
    "\n",
    "async def throttled_concurrency_test():\n",
    "    print(f\"\\n--- Test de Concurrence avec Délai (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [\n",
    "            send_request_with_delay(session, TEST_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # Analyse des Résultats\n",
    "    latencies = [r[0] for r in results]\n",
    "    avg_latency = sum(latencies) / TOTAL_REQUESTS\n",
    "    rps = TOTAL_REQUESTS / duration_total\n",
    "    \n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Débit (RPS) : {rps:.2f}\")\n",
    "    print(f\"Latence Moyenne : {avg_latency * 1000:.2f}ms\")\n",
    "\n",
    "await throttled_concurrency_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fd156",
   "metadata": {},
   "source": [
    "## Mesurer le taux d'erreur (codes 5xx) et la capacité du système à se stabiliser après un pic de charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84bedad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test de Concurrence avec Évaluation des Erreurs (Total: 1000 | Simultanéité: 500) ---\n",
      "Durée totale : 7.53s\n",
      "Taux de Succès : 100.00%\n",
      "Erreurs Serveur (5xx) : 0\n",
      "Erreurs Client (4xx) : 0\n"
     ]
    }
   ],
   "source": [
    "async def resilience_concurrency_test():\n",
    "    print(f\"\\n--- Test de Concurrence avec Évaluation des Erreurs (Total: {TOTAL_REQUESTS} | Simultanéité: {MAX_CONCURRENCY}) ---\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    start_total = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [\n",
    "            send_request_with_semaphore(session, TEST_TEXT, semaphore) \n",
    "            for _ in range(TOTAL_REQUESTS)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    duration_total = time.time() - start_total\n",
    "    \n",
    "    # --- Analyse des Résultats ---\n",
    "    statuses = [r[1] for r in results]\n",
    "    \n",
    "    # Compter les erreurs\n",
    "    errors_5xx = statuses.count(500) + statuses.count(502) + statuses.count(503)\n",
    "    errors_4xx = statuses.count(400) + statuses.count(404)\n",
    "    \n",
    "    print(f\"Durée totale : {duration_total:.2f}s\")\n",
    "    print(f\"Taux de Succès : {100 - (errors_5xx + errors_4xx) / TOTAL_REQUESTS * 100:.2f}%\")\n",
    "    print(f\"Erreurs Serveur (5xx) : {errors_5xx}\")\n",
    "    print(f\"Erreurs Client (4xx) : {errors_4xx}\")\n",
    "\n",
    "await resilience_concurrency_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a264b17",
   "metadata": {},
   "source": [
    "## 1. Montée en Charge Progressive (Ramp-up Test)\n",
    "Objectif\n",
    "Évaluation\n",
    "Mesurer comment le système réagit à une augmentation constante et progressive de la charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7880f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Montée en Charge Progressive ---\n",
      "\n",
      "Charge actuelle: 50 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 250/250\n",
      "  -> RPS: 155.06\n",
      "  -> Latence Moyenne: 280.93ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 100 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 250/250\n",
      "  -> RPS: 155.06\n",
      "  -> Latence Moyenne: 280.93ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 100 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 500/500\n",
      "  -> RPS: 109.90\n",
      "  -> Latence Moyenne: 818.67ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 150 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 500/500\n",
      "  -> RPS: 109.90\n",
      "  -> Latence Moyenne: 818.67ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 150 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 750/750\n",
      "  -> RPS: 149.12\n",
      "  -> Latence Moyenne: 948.86ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 200 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 750/750\n",
      "  -> RPS: 149.12\n",
      "  -> Latence Moyenne: 948.86ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 200 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 1000/1000\n",
      "  -> RPS: 127.54\n",
      "  -> Latence Moyenne: 1463.62ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 250 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 1000/1000\n",
      "  -> RPS: 127.54\n",
      "  -> Latence Moyenne: 1463.62ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 250 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 1250/1250\n",
      "  -> RPS: 136.98\n",
      "  -> Latence Moyenne: 1615.80ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 300 utilisateurs pendant 10s\n",
      "  -> Requêtes complétées: 1250/1250\n",
      "  -> RPS: 136.98\n",
      "  -> Latence Moyenne: 1615.80ms\n",
      "  -> Erreurs: 0\n",
      "\n",
      "Charge actuelle: 300 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 350 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 350 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 400 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 400 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 450 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 450 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 500 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "\n",
      "Charge actuelle: 500 utilisateurs pendant 10s\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n",
      "  Timeout après 10s\n",
      "  -> Aucune requête complétée\n"
     ]
    }
   ],
   "source": [
    "async def progressive_ramp_up_test(max_concurrency=500, step=50, step_duration=10):\n",
    "    \"\"\"Augmente la charge de 50 utilisateurs toutes les 10 secondes jusqu'à 500.\"\"\"\n",
    "    print(\"\\n--- 1. Montée en Charge Progressive ---\")\n",
    "    \n",
    "    current_concurrency = 0\n",
    "    while current_concurrency <= max_concurrency:\n",
    "        current_concurrency += step\n",
    "        if current_concurrency > max_concurrency:\n",
    "            current_concurrency = max_concurrency\n",
    "            \n",
    "        print(f\"\\nCharge actuelle: {current_concurrency} utilisateurs pendant {step_duration}s\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        semaphore = asyncio.Semaphore(current_concurrency)\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Créer un grand nombre de tâches pour s'assurer que le sémaphore est toujours plein\n",
    "            tasks = [send_request_with_semaphore(session, \"Ramp-up test.\", semaphore) for _ in range(current_concurrency * 5)]\n",
    "            \n",
    "            # Utiliser asyncio.gather au lieu de asyncio.wait pour obtenir les résultats\n",
    "            try:\n",
    "                results = await asyncio.wait_for(asyncio.gather(*tasks, return_exceptions=True), timeout=step_duration)\n",
    "            except asyncio.TimeoutError:\n",
    "                print(f\"  Timeout après {step_duration}s\")\n",
    "                results = []\n",
    "            \n",
    "        # Analyser les métriques (RPS, Latence, Erreurs)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Extraire les latences et les statuts des résultats\n",
    "        latencies = []\n",
    "        statuses = []\n",
    "        errors = 0\n",
    "        for r in results:\n",
    "            if isinstance(r, tuple) and len(r) == 2:\n",
    "                latencies.append(r[0])\n",
    "                statuses.append(r[1])\n",
    "                if r[1] >= 400:  # erreur 4xx ou 5xx\n",
    "                    errors += 1\n",
    "            elif isinstance(r, Exception):\n",
    "                errors += 1\n",
    "        \n",
    "        if latencies:\n",
    "            avg_latency = sum(latencies) / len(latencies)\n",
    "            rps = len(latencies) / duration if duration > 0 else 0\n",
    "            print(f\"  -> Requêtes complétées: {len(latencies)}/{current_concurrency * 5}\")\n",
    "            print(f\"  -> RPS: {rps:.2f}\")\n",
    "            print(f\"  -> Latence Moyenne: {avg_latency * 1000:.2f}ms\")\n",
    "            print(f\"  -> Erreurs: {errors}\")\n",
    "        else:\n",
    "            print(f\"  -> Aucune requête complétée\")\n",
    "        \n",
    "        if current_concurrency == max_concurrency:\n",
    "            break\n",
    "\n",
    "await progressive_ramp_up_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3e9fd",
   "metadata": {},
   "source": [
    "## Détecter les fuites de mémoire, la dégradation des performances ou les problèmes de garbage collection sur une longue période."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Utilisez le code soak_test de la réponse précédente en fixant duration_minutes=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005aaf3c",
   "metadata": {},
   "source": [
    "## Vérifier la capacité du système à absorber un pic de charge soudain et à se rétablir rapidement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e118c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Burst Trafic Soudain (Spike Test) ---\n",
      "Phase 1: Spike de 1000 requêtes...\n",
      "Spike terminé en 7.62s\n",
      "Phase 2: Vérification de la récupération (50 requêtes)...\n",
      "Spike terminé en 7.62s\n",
      "Phase 2: Vérification de la récupération (50 requêtes)...\n",
      "Récupération terminée en 0.31s\n",
      "Temps de récupération par requête: 6.30ms\n",
      "Récupération terminée en 0.31s\n",
      "Temps de récupération par requête: 6.30ms\n"
     ]
    }
   ],
   "source": [
    "async def spike_test(spike_concurrency=1000, normal_concurrency=50):\n",
    "    \"\"\"Simule un pic soudain de 1000 requêtes.\"\"\"\n",
    "    print(\"\\n--- 3. Burst Trafic Soudain (Spike Test) ---\")\n",
    "    \n",
    "    # Phase 1 : Le Spike (1000 requêtes simultanées)\n",
    "    print(f\"Phase 1: Spike de {spike_concurrency} requêtes...\")\n",
    "    start_spike = time.time()\n",
    "    semaphore_spike = asyncio.Semaphore(spike_concurrency)\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request_with_semaphore(session, \"Spike payload.\", semaphore_spike) for _ in range(spike_concurrency)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_spike = time.time() - start_spike\n",
    "    print(f\"Spike terminé en {duration_spike:.2f}s\")\n",
    "    \n",
    "    # Phase 2 : Récupération (Charge nominale)\n",
    "    print(f\"Phase 2: Vérification de la récupération ({normal_concurrency} requêtes)...\")\n",
    "    start_recovery = time.time()\n",
    "    semaphore_recovery = asyncio.Semaphore(normal_concurrency)\n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request_with_semaphore(session, \"Recovery payload.\", semaphore_recovery) for _ in range(normal_concurrency)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    duration_recovery = time.time() - start_recovery\n",
    "    print(f\"Récupération terminée en {duration_recovery:.2f}s\")\n",
    "    \n",
    "    # Évaluation : La latence de la Phase 2 doit être proche de la latence normale.\n",
    "    print(f\"Temps de récupération par requête: {duration_recovery / normal_concurrency * 1000:.2f}ms\")\n",
    "\n",
    "await spike_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5289b2",
   "metadata": {},
   "source": [
    "## 8. Traitement Payloads Lourds (Large JSON)\n",
    "Objectif\n",
    "\n",
    "Code (Logique)\n",
    "Mesurer l'impact du transfert de données volumineuses (bien que le texte soit généralement petit, nous simulons un grand corps de requête)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00f1f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Traitement Payloads Lourds ---\n",
      "100 requêtes de 1 Mo exécutées en 14.83s\n",
      "100 requêtes de 1 Mo exécutées en 14.83s\n"
     ]
    }
   ],
   "source": [
    "async def large_payload_test(n_requests=100):\n",
    "    \"\"\"Teste l'impact d'un corps de requête volumineux.\"\"\"\n",
    "    print(\"\\n--- 8. Traitement Payloads Lourds ---\")\n",
    "    \n",
    "    # Créer un texte de 1 Mo (environ 1 million de caractères)\n",
    "    large_text = \"A\" * 1000000 \n",
    "    \n",
    "    start_time = time.time()\n",
    "    semaphore = asyncio.Semaphore(10) # Limiter la concurrence pour ne pas saturer\n",
    "    \n",
    "    async with aiohttp.ClientSession( ) as session:\n",
    "        tasks = [send_request_with_semaphore(session, large_text, semaphore) for _ in range(n_requests)]\n",
    "        await asyncio.gather(*tasks)\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{n_requests} requêtes de 1 Mo exécutées en {duration:.2f}s\")\n",
    "    \n",
    "    # Évaluation : La latence doit rester acceptable. Surveiller l'utilisation de la mémoire dans Cloud Monitoring.\n",
    "\n",
    "await large_payload_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
