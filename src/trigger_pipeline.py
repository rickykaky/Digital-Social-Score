"""
Trigger Pipeline - Soumet le pipeline Kubeflow √† Vertex AI Pipelines
"""

import argparse
import json
import logging
from pathlib import Path
from typing import Optional

from google.cloud import aiplatform
from kfp import compiler, dsl
from kfp.dsl import Input, Model, Output, component, Metrics, Artifact
from typing import NamedTuple

# D√©finir le type de retour pour l'√©valuation
ModelEvaluation = NamedTuple("ModelEvaluation", [
    ("accuracy", float), 
    ("precision", float), 
    ("recall", float), 
    ("f1_score", float), 
    ("deploy_decision", str)
])

# Configuration logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# --- COMPOSANTS KFP ---


@component(
    base_image="python:3.11",
    packages_to_install=["pandas", "nltk", "scikit-learn", "joblib"],
)
def prepare_data_op(raw_csv_path: str, clean_csv_path: str):
    """Pr√©pare les donn√©es pour l'entra√Ænement."""
    import re

    import nltk
    import pandas as pd
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import word_tokenize

    logger.info(f"Chargement des donn√©es depuis {raw_csv_path}")

    # T√©l√©charger les ressources NLTK
    nltk.download("punkt")
    nltk.download("stopwords")
    nltk.download("wordnet")

    # Charger les donn√©es
    df = pd.read_csv(raw_csv_path)
    logger.info(f"Donn√©es charg√©es: {len(df)} lignes")

    # Supprimer les valeurs manquantes
    df = df.dropna(subset=["comment_text"])
    df = df[df["comment_text"].str.strip() != ""]

    # Nettoyage du texte
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words("english"))

    def clean_text(text):
        # Supprimer caract√®res sp√©ciaux
        text = re.sub(r"[^a-zA-Z\s]", "", text.lower())
        # Tokenisation
        tokens = word_tokenize(text)
        # Suppression stopwords + lemmatisation
        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
        return " ".join(tokens)

    df["comment_text_clean"] = df["comment_text"].apply(clean_text)

    # Sauvegarder
    df.to_csv(clean_csv_path, index=False)
    logger.info(f"Donn√©es nettoy√©es sauvegard√©es: {clean_csv_path}")


@component(
    base_image="python:3.11", packages_to_install=["pandas", "scikit-learn", "joblib"]
)
def train_model_op(
    clean_csv_path: str,
    model_path: Output[Model],
    vectorizer_path: str = "gs://digital-social-score/models/vectorizer.joblib",
):
    """Entra√Æne le mod√®le de toxicit√©."""
    import joblib
    import pandas as pd
    from google.cloud import storage
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression

    logger.info(f"Chargement des donn√©es depuis {clean_csv_path}")
    df = pd.read_csv(clean_csv_path)

    # Pr√©parer X et y
    X = df["comment_text_clean"].fillna("")
    y = df["toxic"]

    logger.info(f"Entra√Ænement avec {len(X)} √©chantillons")

    # Vectorisation TF-IDF
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X_vec = vectorizer.fit_transform(X)

    # Entra√Ænement
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_vec, y)

    logger.info("Mod√®le entra√Æn√© avec succ√®s")

    # Sauvegarder localement
    joblib.dump(model, model_path.path)
    joblib.dump(vectorizer, "/tmp/vectorizer.joblib")

    # Upload vers GCS (optionnel)
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket("digital-social-score")

        # Upload mod√®le
        blob_model = bucket.blob("models/model.joblib")
        blob_model.upload_from_filename(model_path.path)

        # Upload vectorizer
        blob_vec = bucket.blob("models/vectorizer.joblib")
        blob_vec.upload_from_filename("/tmp/vectorizer.joblib")

        logger.info("Mod√®le upload√© vers GCS")
    except Exception as e:
        logger.warning(f"Impossible d'uploader vers GCS: {e}")


@component(
    base_image="python:3.11", 
    packages_to_install=["pandas", "scikit-learn", "joblib", "google-cloud-storage"],
)
def evaluate_model_op(
    model_path: Input[Model], 
    vectorizer_path: str,
    clean_csv_path: str,
    metrics: Output[Metrics]
) -> ModelEvaluation:
    """√âvalue le mod√®le et retourne les m√©triques + d√©cision de d√©ploiement."""
    import joblib
    import pandas as pd
    from sklearn.metrics import (accuracy_score, f1_score, precision_score,
                                 recall_score, classification_report)
    from google.cloud import storage
    import os
    import tempfile

    logger.info("üîç √âvaluation du mod√®le")

    # Charger donn√©es de test
    df = pd.read_csv(clean_csv_path)
    X_test = df["comment_text_clean"].fillna("")
    y_test = df["toxic"]

    logger.info(f"üìä Donn√©es de test: {len(X_test)} √©chantillons")

    # Charger mod√®le depuis l'artifact
    model = joblib.load(model_path.path)
    logger.info("‚úÖ Mod√®le charg√©")

    # Charger vectorizer depuis GCS
    try:
        storage_client = storage.Client()
        bucket_name, blob_path = vectorizer_path.replace("gs://", "").split("/", 1)
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            blob.download_to_filename(tmp_file.name)
            vectorizer = joblib.load(tmp_file.name)
            os.unlink(tmp_file.name)
        
        logger.info("‚úÖ Vectorizer charg√© depuis GCS")
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement vectorizer: {e}")
        raise

    # Vectorisation des donn√©es de test
    X_test_vec = vectorizer.transform(X_test)

    # Pr√©dictions
    y_pred = model.predict(X_test_vec)
    y_pred_proba = model.predict_proba(X_test_vec)[:, 1]  # Probabilit√©s classe positive

    # Calculer m√©triques
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    # Log des m√©triques dans Kubeflow
    metrics.log_metric("accuracy", accuracy)
    metrics.log_metric("precision", precision)
    metrics.log_metric("recall", recall)
    metrics.log_metric("f1_score", f1)

    # Log d√©taill√©
    logger.info(f"üìä M√âTRIQUES DU MOD√àLE:")
    logger.info(f"   üéØ Accuracy:  {accuracy:.4f}")
    logger.info(f"   üéØ Precision: {precision:.4f}")
    logger.info(f"   üéØ Recall:    {recall:.4f}")
    logger.info(f"   üéØ F1-Score:  {f1:.4f}")

    # Classification report
    report = classification_report(y_test, y_pred)
    logger.info(f"üìã Rapport de classification:\n{report}")

    # D√âCISION DE D√âPLOIEMENT BAS√âE SUR L'ACCURACY
    deploy_threshold = 0.85
    deploy_decision = accuracy >= deploy_threshold
    
    if deploy_decision:
        logger.info(f"‚úÖ D√âPLOIEMENT AUTORIS√â: Accuracy {accuracy:.4f} ‚â• {deploy_threshold}")
        decision_str = "true"
    else:
        logger.info(f"‚ùå D√âPLOIEMENT REFUS√â: Accuracy {accuracy:.4f} < {deploy_threshold}")
        decision_str = "false"

    # Retourner les m√©triques et la d√©cision
    return ModelEvaluation(
        accuracy=float(accuracy),
        precision=float(precision), 
        recall=float(recall),
        f1_score=float(f1),
        deploy_decision=decision_str
    )


@component(
    base_image="python:3.11",
    packages_to_install=["google-cloud-build"],
)
def build_and_deploy_docker_op(
    project_id: str,
    region: str = "us-west1",
    cluster_name: str = "social-score-cluster", 
    zone: str = "us-west1-a",
    image_tag: str = "ml-auto-deploy",
) -> str:
    """D√©clenche le build et d√©ploiement automatique via Cloud Build."""
    import logging
    from datetime import datetime
    from google.cloud import cloudbuild_v1

    # Configuration
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    full_tag = f"{image_tag}-{timestamp}"
    
    logging.info(f"ÔøΩ D√âPLOIEMENT ML AUTOMATIQUE")
    logging.info(f"   Accuracy ‚â• 0.85 ‚Üí D√©ploiement autoris√©!")
    logging.info(f"   Projet: {project_id}")
    logging.info(f"   Tag: {full_tag}")
    logging.info(f"   Cluster: {cluster_name} ({zone})")

    try:
        # Cr√©er le client Cloud Build
        client = cloudbuild_v1.CloudBuildClient()
        project_path = f"projects/{project_id}"

        # Configuration du build avec d√©ploiement
        build_config = cloudbuild_v1.Build(
            steps=[
                # √âtape 1: Build Docker avec nouveau mod√®le
                cloudbuild_v1.BuildStep(
                    name="gcr.io/cloud-builders/docker",
                    args=[
                        "build",
                        "-f", "src/Dockerfile.optimized",
                        "-t", f"gcr.io/{project_id}/digital-social-score:{full_tag}",
                        "-t", f"gcr.io/{project_id}/digital-social-score:latest-ml",
                        "."
                    ]
                ),
                # √âtape 2: Push vers Container Registry
                cloudbuild_v1.BuildStep(
                    name="gcr.io/cloud-builders/docker",
                    args=[
                        "push", "--all-tags",
                        f"gcr.io/{project_id}/digital-social-score"
                    ]
                ),
                # √âtape 3: D√©ploiement GKE automatique
                cloudbuild_v1.BuildStep(
                    name="gcr.io/google.com/cloudsdktool/cloud-sdk",
                    entrypoint="bash",
                    args=[
                        "-c",
                        f"""
                        echo "üöÄ D√©ploiement ML automatique avec nouveau mod√®le..."
                        gcloud container clusters get-credentials {cluster_name} --zone {zone} --project {project_id}
                        kubectl get namespace production || kubectl create namespace production
                        kubectl set image deployment/social-score-api social-score-api=gcr.io/{project_id}/digital-social-score:{full_tag} -n production
                        kubectl rollout status deployment/social-score-api -n production --timeout=10m
                        echo "‚úÖ D√©ploiement ML automatique termin√©!"
                        echo "üìä Image d√©ploy√©e: {full_tag}"
                        """
                    ]
                )
            ],
            images=[
                f"gcr.io/{project_id}/digital-social-score:{full_tag}",
                f"gcr.io/{project_id}/digital-social-score:latest-ml"
            ],
            substitutions={
                "_REGION": region,
                "_ZONE": zone,
                "_CLUSTER_NAME": cluster_name,
                "_TAG": full_tag
            }
        )

        # Lancer le build (asynchrone)
        logging.info("üî® Lancement du build Cloud Build...")
        operation = client.create_build(
            parent=project_path,
            build=build_config
        )
        
        build_id = operation.metadata.build.id
        logging.info(f"‚úÖ Build lanc√© avec succ√®s!")
        logging.info(f"   Build ID: {build_id}")
        
        build_url = f"https://console.cloud.google.com/cloud-build/builds/{build_id}?project={project_id}"
        logging.info(f"üìä Suivre le build: {build_url}")
        
        return f"SUCCESS: Build automatique {full_tag} lanc√© (ID: {build_id})"
        
    except Exception as e:
        logging.error(f"‚ùå Erreur d√©ploiement automatique: {e}")
        return f"FAILED: {str(e)}"


# --- PIPELINE ---


@dsl.pipeline(
    name="digital-social-score-pipeline",
    description="Pipeline ML avec d√©ploiement automatique conditionnel (accuracy ‚â• 0.85)",
    pipeline_root="gs://digital-social-score/pipeline-root",
)
def digital_score_pipeline(
    raw_csv_path: str = "gs://digital-social-score/data/train.csv",
    clean_csv_path: str = "gs://digital-social-score/data/clean.csv",
    project_id: str = "digital-social-score",
    region: str = "us-west1",
    cluster_name: str = "social-score-cluster",
    zone: str = "us-west1-a",
    deploy_threshold: float = 0.85,
):
    """
    Pipeline ML complet avec d√©ploiement conditionnel:
    1. Pr√©paration des donn√©es
    2. Entra√Ænement du mod√®le NLTK
    3. √âvaluation (accuracy, precision, recall, f1)
    4. SI accuracy ‚â• 0.85 ‚Üí D√©ploiement automatique Docker + GKE
    5. SINON ‚Üí Pas de d√©ploiement
    """

    # ========================================================================
    # √âTAPE 1: Pr√©paration des donn√©es
    # ========================================================================
    prepare_task = prepare_data_op(
        raw_csv_path=raw_csv_path, 
        clean_csv_path=clean_csv_path
    )
    prepare_task.set_display_name("üìã Pr√©paration des donn√©es")

    # ========================================================================
    # √âTAPE 2: Entra√Ænement du mod√®le
    # ========================================================================
    train_task = train_model_op(
        clean_csv_path=clean_csv_path
    )
    train_task.after(prepare_task)
    train_task.set_display_name("ü§ñ Entra√Ænement NLTK")

    # ========================================================================
    # √âTAPE 3: √âvaluation avec d√©cision de d√©ploiement
    # ========================================================================
    eval_task = evaluate_model_op(
        model_path=train_task.outputs["model_path"],
        vectorizer_path="gs://digital-social-score/models/vectorizer.joblib",
        clean_csv_path=clean_csv_path
    )
    eval_task.after(train_task)
    eval_task.set_display_name("üìä √âvaluation du mod√®le")

    # ========================================================================
    # √âTAPE 4: D√©ploiement conditionnel (SI accuracy ‚â• 0.85)
    # ========================================================================
    with dsl.If(
        eval_task.outputs["deploy_decision"] == "true",  # decision_str from evaluate_model_op
        name="deploy_condition"
    ):
        deploy_task = build_and_deploy_docker_op(
            project_id=project_id,
            region=region,
            cluster_name=cluster_name,
            zone=zone,
            image_tag=f"ml-auto-v{deploy_threshold}"
        )
        deploy_task.set_display_name("üöÄ D√©ploiement automatique")


# --- FONCTIONS DE SOUMISSION ---


def compile_pipeline(output_path: str = "digital_score_pipeline.yaml") -> str:
    """Compile le pipeline en YAML."""
    logger.info(f"Compilation du pipeline...")
    compiler.Compiler().compile(
        pipeline_func=digital_score_pipeline, package_path=output_path
    )
    logger.info(f"Pipeline compil√©: {output_path}")
    return output_path


def submit_pipeline(
    project_id: str,
    region: str = "us-west1",
    pipeline_yaml: str = "digital_score_pipeline.yaml",
    display_name: str = "Digital-Social-Score-ML-Pipeline",
    cluster_name: str = "social-score-cluster",
    zone: str = "us-west1-a",
    deploy_threshold: float = 0.85,
) -> str:
    """Soumet le pipeline ML avec d√©ploiement conditionnel √† Vertex AI."""

    logger.info(f"üöÄ Initialisation Vertex AI ML Pipeline")
    logger.info(f"   Projet: {project_id}")
    logger.info(f"   R√©gion: {region}")
    logger.info(f"   Seuil d√©ploiement: accuracy ‚â• {deploy_threshold}")
    
    aiplatform.init(project=project_id, location=region)

    # Param√®tres du pipeline avec d√©ploiement conditionnel
    pipeline_params = {
        "raw_csv_path": "gs://digital-social-score/data/train.csv",
        "clean_csv_path": "gs://digital-social-score/data/clean.csv",
        "project_id": project_id,
        "region": region,
        "cluster_name": cluster_name,
        "zone": zone,
        "deploy_threshold": deploy_threshold,
    }

    logger.info(f"üìã Param√®tres du pipeline:")
    for key, value in pipeline_params.items():
        logger.info(f"   {key}: {value}")

    # Cr√©er le job pipeline
    job = aiplatform.PipelineJob(
        display_name=display_name,
        template_path=pipeline_yaml,
        pipeline_root="gs://digital-social-score/pipeline-root",
        parameter_values=pipeline_params,
        enable_caching=True,  # Cache pour optimiser les re-runs
    )

    logger.info(f"üîÑ Soumission du pipeline ML...")
    job.submit()
    
    logger.info(f"‚úÖ Pipeline ML soumis avec succ√®s!")
    logger.info(f"   Job ID: {job.name}")
    logger.info(f"   Display Name: {display_name}")
    logger.info(f"üìä Console Vertex AI:")
    logger.info(f"   https://console.cloud.google.com/vertex-ai/pipelines")
    logger.info(f"üí° Le mod√®le sera d√©ploy√© automatiquement si accuracy ‚â• {deploy_threshold}")

    return job.name


def main():
    parser = argparse.ArgumentParser(
        description="Pipeline ML Digital Social Score avec d√©ploiement automatique conditionnel"
    )
    parser.add_argument("--project", required=True, help="Google Cloud Project ID")
    parser.add_argument("--region", default="us-west1", help="GCP Region (d√©faut: us-west1)")
    parser.add_argument(
        "--compile-only",
        action="store_true",
        help="Compiler seulement, ne pas soumettre",
    )
    parser.add_argument(
        "--yaml",
        default="digital_score_pipeline.yaml",
        help="Chemin du fichier YAML compil√©",
    )
    parser.add_argument(
        "--display-name",
        default="Digital-Social-Score-ML-Pipeline",
        help="Nom du pipeline",
    )
    parser.add_argument(
        "--cluster-name",
        default="social-score-cluster",
        help="Nom du cluster GKE pour d√©ploiement",
    )
    parser.add_argument(
        "--zone",
        default="us-west1-a",
        help="Zone du cluster GKE",
    )
    parser.add_argument(
        "--deploy-threshold",
        type=float,
        default=0.85,
        help="Seuil d'accuracy pour d√©ploiement automatique (d√©faut: 0.85)",
    )

    args = parser.parse_args()

    try:
        logger.info(f"ü§ñ PIPELINE ML DIGITAL SOCIAL SCORE")
        logger.info(f"   Mode: {'COMPILATION SEULEMENT' if args.compile_only else 'COMPILATION + SOUMISSION'}")
        logger.info(f"   Projet: {args.project}")
        logger.info(f"   Seuil d√©ploiement: accuracy ‚â• {args.deploy_threshold}")

        # Compiler le pipeline
        yaml_path = compile_pipeline(args.yaml)

        if args.compile_only:
            logger.info(f"‚úÖ Pipeline compil√© uniquement: {yaml_path}")
            return

        # Soumettre le pipeline avec d√©ploiement conditionnel
        job_name = submit_pipeline(
            project_id=args.project,
            region=args.region,
            pipeline_yaml=yaml_path,
            display_name=args.display_name,
            cluster_name=args.cluster_name,
            zone=args.zone,
            deploy_threshold=args.deploy_threshold,
        )

        logger.info(f"üéâ SUCC√àS!")
        logger.info(f"   Pipeline Job: {job_name}")
        logger.info(f"   Le mod√®le sera d√©ploy√© automatiquement si accuracy ‚â• {args.deploy_threshold}")
        logger.info(f"üìä Suivez l'ex√©cution sur Vertex AI Pipelines Console")

    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}", exc_info=True)
        exit(1)


if __name__ == "__main__":
    main()
