# Pipeline Cloud Build Optimis√© avec Entra√Ænement NLTK
# S√©quence: Tests ‚Üí Entra√Ænement ‚Üí Build Docker ‚Üí D√©ploiement Asynchrone
# central1
steps:
  # ============================================================================
  # √âTAPE PARALL√àLE: R√©cup√©ration Cache (ne bloque pas)
  # ============================================================================
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'restore-cache'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "üíæ Tentative de restauration du cache uv..."
        gsutil cp gs://${PROJECT_ID}-build-cache/uv-cache-${_CACHE_VERSION}.tar.gz . || echo "Aucun cache trouv√©"
        gsutil cp gs://${PROJECT_ID}-build-cache/nltk-cache-${_CACHE_VERSION}.tar.gz . || echo "Aucun cache NLTK trouv√©"
        gsutil cp gs://${PROJECT_ID}-build-cache/model-cache-${_CACHE_VERSION}.tar.gz . || echo "Aucun cache mod√®le trouv√©"
        echo "‚úÖ √âtape cache termin√©e"
    waitFor: ['-']  # D√©marrage imm√©diat en parall√®le

  # ============================================================================
  # √âTAPE 0: Tests avec Cache uv
  # ============================================================================
  - name: python:3.11
    id: 'tests'
    entrypoint: bash
    args:
      - -c
      - |
        echo "üìã √âtape 0: Tests avec cache uv optimis√©..."
        
        # Restaurer le cache uv si disponible
        if [ -f "uv-cache-${_CACHE_VERSION}.tar.gz" ]; then
          echo "üì¶ Restauration cache uv..."
          mkdir -p ~/.cache ~/.local/share
          tar -xzf uv-cache-${_CACHE_VERSION}.tar.gz -C ~ 2>/dev/null || echo "Cache uv non restaurable"
        fi
        
        # Installation ultra-rapide avec uv
        echo "‚ö° Installation avec uv (cached)..."
        pip install uv
        time uv pip install --system -r requirements.txt
        time uv pip install --system -r requirements-test.txt
        
        # Tests
        echo "üß™ Ex√©cution des tests..."
        python -m pytest tests/ -v --tb=short || echo "Tests √©chou√©s mais build continue"
        echo "‚úÖ Tests termin√©s!"
    waitFor: ['restore-cache']

  # ============================================================================
  # √âTAPE 1: Entra√Ænement du Mod√®le NLTK (CRITIQUE - AVANT DOCKER)
  # ============================================================================
  - name: python:3.11
    id: 'train-model'
    entrypoint: bash
    args:
      - -c
      - |
        echo "ü§ñ √âTAPE 1: Entra√Ænement du mod√®le NLTK..."
        
        # Restaurer le cache uv
        if [ -f "uv-cache-${_CACHE_VERSION}.tar.gz" ]; then
          echo "üì¶ Restauration cache uv..."
          mkdir -p ~/.cache ~/.local/share
          tar -xzf uv-cache-${_CACHE_VERSION}.tar.gz -C ~ 2>/dev/null || echo "Cache uv non restaurable"
        fi
        
        # Restaurer le cache NLTK si disponible  
        if [ -f "nltk-cache-${_CACHE_VERSION}.tar.gz" ]; then
          echo "üì• Restauration cache NLTK..."
          mkdir -p ~/nltk_data /usr/share/nltk_data
          tar -xzf nltk-cache-${_CACHE_VERSION}.tar.gz -C ~ 2>/dev/null || echo "Cache NLTK non restaurable"
          cp -r ~/nltk_data/* /usr/share/nltk_data/ 2>/dev/null || echo "Copie NLTK data"
        fi
        
        # Installation des d√©pendances pour l'entra√Ænement
        echo "üì¶ Installation des d√©pendances d'entra√Ænement..."
        pip install uv
        uv pip install --system -r requirements.txt
        
        # T√©l√©charger donn√©es NLTK si pas en cache
        echo "üìö V√©rification des donn√©es NLTK..."
        export NLTK_DATA=/usr/share/nltk_data
        python -m nltk.downloader -d /usr/share/nltk_data punkt punkt_tab stopwords wordnet averaged_perceptron_tagger maxent_ne_chunker words vader_lexicon
        
        # Restaurer un mod√®le existant si disponible
        if [ -f "model-cache-${_CACHE_VERSION}.tar.gz" ]; then
          echo "üîÑ Restauration mod√®le existant..."
          mkdir -p src/models
          tar -xzf model-cache-${_CACHE_VERSION}.tar.gz -C src/ 2>/dev/null || echo "Cache mod√®le non restaurable"
        fi
        
        # ENTRA√éNEMENT DU MOD√àLE sur cleaned_training_sample.csv
        echo "üéØ Lancement de l'entra√Ænement sur cleaned_training_sample.csv..."
        cd src
        
        # V√©rifier que le fichier de donn√©es existe
        if [ -f "../data/cleaned_training_sample.csv" ]; then
          echo "‚úÖ Fichier de donn√©es trouv√©: cleaned_training_sample.csv"
          
          # Lancer l'entra√Ænement avec le fichier sp√©cifique
          python -c "import sys; sys.path.append('.'); from train import train_and_save_model; print('üöÄ D√©marrage entra√Ænement'); train_and_save_model('../data/cleaned_training_sample.csv'); print('‚úÖ Entra√Ænement termin√©!')"
        else
          echo "‚ö†Ô∏è  Fichier cleaned_training_sample.csv non trouv√©, utilisation des donn√©es par d√©faut"
          python train.py
        fi
        
        # V√©rifier que le mod√®le a √©t√© cr√©√©
        echo "üîç V√©rification des mod√®les cr√©√©s..."
        echo "R√©pertoire de travail : $(pwd)"
        find . -name "*.joblib" -type f 2>/dev/null | head -10 || echo "Aucun fichier .joblib trouv√©"
        
        # Chercher dans diff√©rents emplacements
        if [ -f "src/models/model.joblib" ] && [ -f "src/models/vectorizer.joblib" ]; then
          echo "‚úÖ Mod√®le entra√Æn√© avec succ√®s dans src/models/!"
          echo "   üìÑ Mod√®le: src/models/model.joblib"
          echo "   üìÑ Vectoriseur: src/models/vectorizer.joblib"
          ls -la src/models/
          
          # Cr√©er l'archive de cache du mod√®le
          echo "üíæ Cr√©ation du cache mod√®le..."
          cd ..
          tar -czf model-cache-${_CACHE_VERSION}.tar.gz src/models/ || echo "Erreur sauvegarde mod√®le"
        elif [ -f "models/model.joblib" ] && [ -f "models/vectorizer.joblib" ]; then
          echo "‚úÖ Mod√®le entra√Æn√© avec succ√®s dans models/!"
          echo "   üìÑ Mod√®le: models/model.joblib"
          echo "   üìÑ Vectoriseur: models/vectorizer.joblib"
          ls -la models/
          
          # D√©placer vers src/models/ pour coh√©rence
          mkdir -p src/models/
          cp models/*.joblib src/models/
          
          echo "üíæ Cr√©ation du cache mod√®le..."
          cd ..
          tar -czf model-cache-${_CACHE_VERSION}.tar.gz src/models/ || echo "Erreur sauvegarde mod√®le"
        else
          echo "‚ùå ERREUR: Mod√®le non cr√©√©!"
          echo "Contenu du r√©pertoire de travail:"
          ls -la
          echo "Recherche de tous les fichiers .joblib:"
          find . -name "*.joblib" -type f 2>/dev/null || echo "Aucun fichier .joblib trouv√©"
          exit 1
        fi
        
        echo "üéâ √âtape d'entra√Ænement termin√©e avec succ√®s!"
    waitFor: ['tests']

  # ============================================================================
  # √âTAPE 2: Construction Docker avec Mod√®le Fra√Æchement Entra√Æn√©
  # ============================================================================
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: 
      - 'build'
      - '-f'
      - 'src/Dockerfile.optimized'
      - '--cache-from'
      - 'gcr.io/$PROJECT_ID/digital-social-score:cache'
      - '--cache-from'
      - 'gcr.io/$PROJECT_ID/digital-social-score:latest'
      - '-t'
      - 'gcr.io/$PROJECT_ID/digital-social-score:${_TAG}'
      - '-t'
      - 'gcr.io/$PROJECT_ID/digital-social-score:latest'
      - '-t'
      - 'gcr.io/$PROJECT_ID/digital-social-score:cache'
      - '.'
    waitFor: ['train-model']  # üéØ CRITIQUE: Attend la fin de l'entra√Ænement

  # ============================================================================
  # √âTAPE 3: Push vers Container Registry
  # ============================================================================
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-push'
    args: 
      - 'push'
      - '--all-tags'
      - 'gcr.io/$PROJECT_ID/digital-social-score'
    waitFor: ['docker-build']

  # ============================================================================
  # √âTAPE PARALL√àLE: Sauvegarde Cache (ne bloque pas le d√©ploiement)
  # ============================================================================
  - name: python:3.11
    id: 'save-cache'
    entrypoint: bash
    args:
      - -c
      - |
        echo "üíæ Sauvegarde du cache pour les prochains builds..."
        
        # Cr√©er les archives de cache
        if [ -d ~/.cache/uv ] || [ -d ~/.local/share/uv ]; then
          echo "üì¶ Sauvegarde cache uv..."
          tar -czf uv-cache-${_CACHE_VERSION}.tar.gz -C ~ .cache/uv .local/share/uv 2>/dev/null || echo "Erreur sauvegarde uv"
        fi
        
        if [ -d ~/nltk_data ]; then
          echo "üìö Sauvegarde cache NLTK..."
          tar -czf nltk-cache-${_CACHE_VERSION}.tar.gz -C ~ nltk_data 2>/dev/null || echo "Erreur sauvegarde NLTK"
        fi
        
        # Uploader vers GCS
        gsutil cp uv-cache-${_CACHE_VERSION}.tar.gz gs://${PROJECT_ID}-build-cache/ 2>/dev/null || echo "Upload cache uv √©chou√©"
        gsutil cp nltk-cache-${_CACHE_VERSION}.tar.gz gs://${PROJECT_ID}-build-cache/ 2>/dev/null || echo "Upload cache NLTK √©chou√©"
        
        # Uploader vers GCS
        gsutil cp uv-cache-${_CACHE_VERSION}.tar.gz gs://${PROJECT_ID}-build-cache/ 2>/dev/null || echo "Upload cache uv √©chou√©"
        gsutil cp nltk-cache-${_CACHE_VERSION}.tar.gz gs://${PROJECT_ID}-build-cache/ 2>/dev/null || echo "Upload cache NLTK √©chou√©"
        gsutil cp model-cache-${_CACHE_VERSION}.tar.gz gs://${PROJECT_ID}-build-cache/ 2>/dev/null || echo "Upload cache mod√®le √©chou√©"
        
        echo "‚úÖ Cache sauvegard√© pour les prochains builds"
    waitFor: ['train-model']  # D√©marre apr√®s l'entra√Ænement

  # ============================================================================
  # √âTAPE 4: D√©ploiement GKE (ASYNCHRONE - d√®s que l'image est pr√™te)
  # ============================================================================
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'deploy-gke'
    entrypoint: bash
    args:
      - -c
      - |
        echo "ÔøΩ √âTAPE 4: D√©ploiement GKE (asynchrone avec nouveau mod√®le)..."
        
        # Configuration kubectl
        gcloud container clusters get-credentials ${_CLUSTER_NAME} \
          --zone ${_ZONE} \
          --project $PROJECT_ID
        
        # Cr√©er namespace si n√©cessaire
        kubectl get namespace production || kubectl create namespace production
        
        # Appliquer les manifestes Kubernetes de production
        echo "üöÄ D√©ploiement automatique de l'API avec l'image fra√Æche..."
        sed "s|PROJECT_ID|$PROJECT_ID|g" deployment/k8s/production-deployment.yaml | \
        sed "s|gcr.io/PROJECT_ID/digital-social-score:latest|gcr.io/$PROJECT_ID/digital-social-score:${_TAG}|g" | \
        kubectl apply -f -
        
        # Mise √† jour de l'image (pour les d√©ploiements existants)
        echo "üì¶ Mise √† jour de l'image avec le nouveau mod√®le..."
        kubectl set image deployment/social-score-api \
          social-score-api=gcr.io/$PROJECT_ID/digital-social-score:${_TAG} \
          -n production
        
        # Attendre le d√©ploiement (non bloquant gr√¢ce au timeout)
        echo "‚è≥ Attente du d√©ploiement..."
        kubectl rollout status deployment/social-score-api \
          -n production \
          --timeout=10m || echo "‚ö†Ô∏è D√©ploiement en cours (timeout atteint)"
        
        # V√©rifier l'√©tat des pods
        echo "üìä √âtat des pods apr√®s d√©ploiement:"
        kubectl get pods -n production -l app=social-score-api
        
        # Obtenir les informations du service
        echo "üåê Informations du service:"
        kubectl get service social-score-service -n production -o wide || echo "Service en cr√©ation..."
        
        # Attendre que le LoadBalancer soit pr√™t et tester l'API
        echo "üîç V√©rification de la sant√© de l'API d√©ploy√©e..."
        
        # Attendre quelques secondes pour que le service soit pr√™t
        sleep 30
        
        # Obtenir l'IP externe du LoadBalancer
        SERVICE_IP=$$(kubectl get service social-score-service -n production -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        
        if [ -n "$$SERVICE_IP" ]; then
          echo "üåê Service accessible √†: http://$$SERVICE_IP"
          
          # Test de sant√© avec retry
          for i in {1..5}; do
            if curl -f --connect-timeout 10 http://$$SERVICE_IP/health; then
              echo "‚úÖ API r√©pond correctement!"
              break
            else
              echo "‚è≥ Tentative $$i/5 - API pas encore pr√™te, attente..."
              sleep 10
            fi
          done
        else
          echo "‚ö†Ô∏è IP du LoadBalancer pas encore assign√©e (normal lors du premier d√©ploiement)"
          echo "üí° Utilisez cette commande pour obtenir l'IP plus tard:"
          echo "   kubectl get service social-score-service -n production"
        fi
        
        echo "üéâ D√âPLOIEMENT AUTOMATIQUE TERMIN√â!"
        echo "üìä R√©sum√© du d√©ploiement:"
        echo "   - Image: gcr.io/$PROJECT_ID/digital-social-score:${_TAG}"
        echo "   - Namespace: production"  
        echo "   - Replicas: 2 (auto-scaling 2-8)"
        echo "   - Health checks: Activ√©s (/health)"
    waitFor: ['docker-push']  # Peut d√©marrer d√®s que l'image est pr√™te

  # ============================================================================
  # √âTAPES ML PIPELINE VERTEX AI (ASYNCHRONE - EN PARALL√àLE)
  # ============================================================================
  - name: python:3.11
    id: 'compile-pipeline'
    dir: '.'
    entrypoint: bash
    args:
      - -c
      - |
        echo "üî® Compilation pipeline ML avec d√©ploiement conditionnel..."
        pip install uv
        uv pip install --system kfp==2.5.0 google-cloud-aiplatform
        cd src
        python trigger_pipeline.py \
          --project $PROJECT_ID \
          --region ${_REGION} \
          --cluster-name ${_CLUSTER_NAME} \
          --zone ${_ZONE} \
          --deploy-threshold 0.85 \
          --compile-only
        echo "‚úÖ Pipeline ML compil√© avec d√©ploiement automatique si accuracy ‚â• 0.85!"
    waitFor: ['docker-push']  # En parall√®le du d√©ploiement GKE

  - name: python:3.11
    id: 'submit-pipeline'
    dir: '.'
    entrypoint: bash
    args:
      - -c
      - |
        echo "üöÄ Soumission pipeline ML avec d√©ploiement conditionnel..."
        pip install uv  
        uv pip install --system kfp==2.5.0 google-cloud-aiplatform
        cd src
        python submit_vertex_pipeline.py \
          --project $PROJECT_ID \
          --region ${_REGION} \
          --yaml digital_score_pipeline.yaml \
          --display-name "Digital-Social-Score-ML-Pipeline-${_TAG}" \
          --async
        echo "‚úÖ Pipeline ML soumis en asynchrone!"
        echo "üìä Le mod√®le sera d√©ploy√© automatiquement si accuracy ‚â• 0.85"
        echo "üìä Consultez l'√©tat du pipeline:"
        echo "   https://console.cloud.google.com/vertex-ai/pipelines/runs"
        echo "üí° Si le mod√®le atteint 85% d'accuracy, une nouvelle image sera build√©e et d√©ploy√©e automatiquement!"
    waitFor: ['compile-pipeline']

# ============================================================================
# IMAGES √Ä CR√âER
# ============================================================================
images:
  - 'gcr.io/$PROJECT_ID/digital-social-score:${_TAG}'
  - 'gcr.io/$PROJECT_ID/digital-social-score:latest'
  - 'gcr.io/$PROJECT_ID/digital-social-score:cache'

# ============================================================================
# CONFIGURATION OPTIMIS√âE
# ============================================================================
options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'N1_HIGHCPU_8'
  substitutionOption: 'ALLOW_LOOSE'
  diskSizeGb: 100
  env:
    - 'CLOUDSDK_COMPUTE_ZONE=${_ZONE}'
    - 'CLOUDSDK_CONTAINER_CLUSTER=${_CLUSTER_NAME}'

timeout: '2400s'  # 40 minutes (plus long pour l'entra√Ænement)

# ============================================================================
# VARIABLES DE SUBSTITUTION
# ============================================================================
substitutions:
  _REGION: 'us-west1'
  _ZONE: 'us-west1-a' 
  _CLUSTER_NAME: 'social-score-cluster'
  _TAG: '${COMMIT_SHA}'
  _CACHE_VERSION: 'v1.2'  # Nouveau cache avec fixes

# ============================================================================
# PERFORMANCE ATTENDUE:
# ============================================================================
# Premier build (cache vide): 15-20 minutes
# Builds suivants (cache hit): 8-12 minutes (40-50% plus rapide)
# Builds code seulement: 5-8 minutes (60-70% plus rapide)
#
# STRAT√âGIES DE CACHE:
# 1. Cache uv ‚Üí D√©pendances Python ultra-rapides
# 2. Cache NLTK ‚Üí √âvite 200MB+ de t√©l√©chargement
# 3. Cache Docker ‚Üí Layers de base r√©utilis√©s
# 4. Ex√©cution parall√®le ‚Üí Sauvegarde cache non bloquante